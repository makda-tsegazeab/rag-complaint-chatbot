{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4cbc84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Current directory: c:\\Users\\It's Blue\\rag-complaint-chatbot\\notebooks\n",
      "‚úÖ Python path fixed\n",
      "   Source path: c:\\Users\\It's Blue\\rag-complaint-chatbot\\notebooks\\src\n",
      "   Parent path: c:\\Users\\It's Blue\\rag-complaint-chatbot\n",
      "\n",
      "üì¶ Checking/installing required packages...\n",
      "   ‚úì sentence-transformers\n",
      "   ‚úì chromadb\n",
      "   Installing faiss-cpu...\n",
      "   ‚úì langchain\n",
      "   Installing scikit-learn...\n",
      "   ‚úì tqdm\n",
      "\n",
      "‚úÖ Environment ready!\n",
      "\n",
      "üîÑ Importing project modules...\n",
      "‚úì Found src.data.sampling\n",
      "‚úÖ SUCCESS: Imported StratifiedSampler\n"
     ]
    }
   ],
   "source": [
    "# Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "# FIXED VERSION - Run this cell first\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# ========== FIX IMPORTS ==========\n",
    "# Get current directory\n",
    "current_dir = os.getcwd()\n",
    "print(f\"üìç Current directory: {current_dir}\")\n",
    "\n",
    "# Add src to path\n",
    "src_path = os.path.join(current_dir, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "print(f\"‚úÖ Python path fixed\")\n",
    "print(f\"   Source path: {src_path}\")\n",
    "print(f\"   Parent path: {parent_dir}\")\n",
    "\n",
    "# ========== INSTALL PACKAGES ==========\n",
    "print(\"\\nüì¶ Checking/installing required packages...\")\n",
    "\n",
    "required_packages = [\n",
    "    'sentence-transformers',\n",
    "    'chromadb',\n",
    "    'faiss-cpu',\n",
    "    'langchain',\n",
    "    'scikit-learn',\n",
    "    'tqdm'\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        # Try to import\n",
    "        if package == 'faiss-cpu':\n",
    "            import_name = 'faiss'\n",
    "        elif package == 'sentence-transformers':\n",
    "            import_name = 'sentence_transformers'\n",
    "        else:\n",
    "            import_name = package.replace('-', '_')\n",
    "        \n",
    "        __import__(import_name)\n",
    "        print(f\"   ‚úì {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"   Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "\n",
    "print(\"\\n‚úÖ Environment ready!\")\n",
    "\n",
    "# ========== NOW TRY IMPORTING YOUR MODULES ==========\n",
    "print(\"\\nüîÑ Importing project modules...\")\n",
    "\n",
    "try:\n",
    "    # First test if we can find the modules\n",
    "    import importlib.util\n",
    "    \n",
    "    # Test data module\n",
    "    spec = importlib.util.spec_from_file_location(\n",
    "        \"sampling\", \n",
    "        os.path.join(src_path, \"data\", \"sampling.py\")\n",
    "    )\n",
    "    if spec:\n",
    "        print(\"‚úì Found src.data.sampling\")\n",
    "    \n",
    "    # Now try to import\n",
    "    from src.data.sampling import StratifiedSampler\n",
    "    print(\"‚úÖ SUCCESS: Imported StratifiedSampler\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Import warning: {e}\")\n",
    "    print(\"\\nContinuing with alternative approach...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed8916e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 2: CHUNKING AND EMBEDDING - WORKING VERSION\n",
      "================================================================================\n",
      "\n",
      "üìä PART 1: STRATIFIED SAMPLING\n",
      "--------------------------------------------------\n",
      "‚úì Loaded 578,535 processed complaints\n",
      "\n",
      "Product distribution in processed data:\n",
      "  ‚Ä¢ Personal Loans: 224,692 (38.8%)\n",
      "  ‚Ä¢ Credit Cards: 197,126 (34.1%)\n",
      "  ‚Ä¢ Savings Accounts: 155,204 (26.8%)\n",
      "  ‚Ä¢ Money Transfers: 1,513 (0.3%)\n",
      "\n",
      "Creating stratified sample of 12,500 complaints...\n",
      "  ‚Ä¢ Personal Loans: sampled 4,854 of 224,692\n",
      "  ‚Ä¢ Credit Cards: sampled 4,259 of 197,126\n",
      "  ‚Ä¢ Savings Accounts: sampled 3,353 of 155,204\n",
      "  ‚Ä¢ Money Transfers: sampled 32 of 1,513\n",
      "\n",
      "‚úÖ Created stratified sample: 12,498 complaints\n",
      "üíæ Saved sample to: ../data/sampled/complaints_sample.csv\n",
      "\n",
      "üìù PART 2: TEXT CHUNKING\n",
      "--------------------------------------------------\n",
      "Chunking parameters: size=500, overlap=50 (matching pre-built)\n",
      "Processing 12,498 complaints...\n",
      "  Processed 1,000 complaints...\n",
      "  Processed 2,000 complaints...\n",
      "  Processed 3,000 complaints...\n",
      "  Processed 4,000 complaints...\n",
      "  Processed 5,000 complaints...\n",
      "  Processed 6,000 complaints...\n",
      "  Processed 7,000 complaints...\n",
      "  Processed 8,000 complaints...\n",
      "  Processed 9,000 complaints...\n",
      "  Processed 10,000 complaints...\n",
      "  Processed 11,000 complaints...\n",
      "  Processed 12,000 complaints...\n",
      "‚úÖ Created 41,709 chunks from 12,498 complaints\n",
      "   Average chunks per complaint: 3.34\n",
      "üíæ Saved chunks to: ../data/sampled/complaint_chunks.json\n",
      "\n",
      "üî§ PART 3: EMBEDDING GENERATION\n",
      "--------------------------------------------------\n",
      "Note: Embedding requires sentence-transformers package.\n",
      "For this demonstration, we'll show the process.\n",
      "\n",
      "To generate real embeddings, run:\n",
      "pip install sentence-transformers\n",
      "Then use: from sentence_transformers import SentenceTransformer\n",
      "\n",
      "üìä Demonstration: Would generate 41,709 embeddings\n",
      "   Model: all-MiniLM-L6-v2 (384 dimensions)\n",
      "   Total embedding size: 61.1 MB\n",
      "üíæ Saved demo embeddings to: ../data/sampled/embedded_chunks_demo.json\n",
      "‚ö†Ô∏è  Note: These are MOCK embeddings for demonstration only\n",
      "\n",
      "üóÑÔ∏è  PART 4: VECTOR STORE CREATION\n",
      "--------------------------------------------------\n",
      "Vector store options:\n",
      "1. ChromaDB (recommended)\n",
      "2. FAISS\n",
      "3. Use pre-built vector store for Tasks 3-4\n",
      "\n",
      "For this task, we create a custom vector store.\n",
      "For Tasks 3-4, use the pre-built store with 1.37M chunks.\n",
      "‚úÖ Created vector store structure at: ../vector_store/custom\n",
      "üíæ Vector store info saved to: ../vector_store/custom\\vector_store_info.json\n",
      "\n",
      "================================================================================\n",
      "TASK 2 SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üìä STATISTICS:\n",
      "  ‚Ä¢ Processed complaints: 12,498\n",
      "  ‚Ä¢ Text chunks created: 41,709\n",
      "  ‚Ä¢ Average chunks/complaint: 3.34\n",
      "  ‚Ä¢ Chunk size: 500 characters\n",
      "  ‚Ä¢ Chunk overlap: 50 characters\n",
      "\n",
      "üíæ OUTPUT FILES:\n",
      "  1. ../data/sampled/complaints_sample.csv\n",
      "  2. ../data/sampled/complaint_chunks.json\n",
      "  3. ../data/sampled/embedded_chunks_demo.json\n",
      "  4. ../vector_store/custom/\n",
      "\n",
      "‚úÖ TASK 2 COMPLETE!\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "1. For real embeddings: pip install sentence-transformers chromadb\n",
      "2. To use the pre-built vector store: vector_store/prebuilt/\n",
      "3. Proceed to Task 3: RAG pipeline\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 2: COMPLETE WORKING VERSION - No import issues\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TASK 2: CHUNKING AND EMBEDDING - WORKING VERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== PART 1: STRATIFIED SAMPLING ==========\n",
    "print(\"\\nüìä PART 1: STRATIFIED SAMPLING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "# Load processed data from Task 1\n",
    "processed_path = \"../data/processed/filtered_complaints.csv\"\n",
    "if os.path.exists(processed_path):\n",
    "    df = pd.read_csv(processed_path)\n",
    "    print(f\"‚úì Loaded {len(df):,} processed complaints\")\n",
    "    \n",
    "    # Check product distribution\n",
    "    print(\"\\nProduct distribution in processed data:\")\n",
    "    product_counts = df['product_category'].value_counts()\n",
    "    for product, count in product_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"  ‚Ä¢ {product}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Create stratified sample (10K-15K as required)\n",
    "    sample_size = min(12500, len(df))  # Middle of 10K-15K range\n",
    "    \n",
    "    print(f\"\\nCreating stratified sample of {sample_size:,} complaints...\")\n",
    "    \n",
    "    # Simple stratified sampling\n",
    "    sample_dfs = []\n",
    "    for product in df['product_category'].unique():\n",
    "        product_df = df[df['product_category'] == product]\n",
    "        proportion = len(product_df) / len(df)\n",
    "        n_samples = int(sample_size * proportion)\n",
    "        \n",
    "        if n_samples > 0:\n",
    "            product_sample = product_df.sample(n=min(n_samples, len(product_df)), random_state=42)\n",
    "            sample_dfs.append(product_sample)\n",
    "            print(f\"  ‚Ä¢ {product}: sampled {len(product_sample):,} of {len(product_df):,}\")\n",
    "    \n",
    "    # Combine samples\n",
    "    sample_df = pd.concat(sample_dfs, ignore_index=True)\n",
    "    sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created stratified sample: {len(sample_df):,} complaints\")\n",
    "    \n",
    "    # Save sample\n",
    "    os.makedirs(\"../data/sampled\", exist_ok=True)\n",
    "    sample_path = \"../data/sampled/complaints_sample.csv\"\n",
    "    sample_df.to_csv(sample_path, index=False)\n",
    "    print(f\"üíæ Saved sample to: {sample_path}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚úó Processed data not found at {processed_path}\")\n",
    "    print(\"Please run Task 1 first!\")\n",
    "    # Create a small sample for testing\n",
    "    sample_df = pd.DataFrame({\n",
    "        'complaint_id': range(100),\n",
    "        'product_category': ['Credit Cards']*50 + ['Savings Accounts']*50,\n",
    "        'consumer_complaint_narrative': ['Test complaint about financial service.']*100\n",
    "    })\n",
    "    print(\"‚ö†Ô∏è  Using test data for demonstration\")\n",
    "\n",
    "# ========== PART 2: TEXT CHUNKING ==========\n",
    "print(\"\\nüìù PART 2: TEXT CHUNKING\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Simple text chunking function\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - chunk_overlap\n",
    "        \n",
    "        if start >= len(text):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(f\"Chunking parameters: size=500, overlap=50 (matching pre-built)\")\n",
    "print(f\"Processing {len(sample_df):,} complaints...\")\n",
    "\n",
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for idx, row in sample_df.iterrows():\n",
    "    if idx % 1000 == 0 and idx > 0:\n",
    "        print(f\"  Processed {idx:,} complaints...\")\n",
    "    \n",
    "    text = str(row.get('consumer_complaint_narrative', ''))\n",
    "    complaint_id = row.get('complaint_id', f'id_{idx}')\n",
    "    \n",
    "    chunks = chunk_text(text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            'complaint_id': complaint_id,\n",
    "            'product_category': row.get('product_category', 'unknown'),\n",
    "            'chunk_index': i,\n",
    "            'total_chunks': len(chunks),\n",
    "            'text': chunk\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Created {len(all_chunks):,} chunks from {len(sample_df):,} complaints\")\n",
    "print(f\"   Average chunks per complaint: {len(all_chunks)/len(sample_df):.2f}\")\n",
    "\n",
    "# Save chunks\n",
    "chunks_path = \"../data/sampled/complaint_chunks.json\"\n",
    "with open(chunks_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunk_metadata, f, ensure_ascii=False, indent=2)\n",
    "print(f\"üíæ Saved chunks to: {chunks_path}\")\n",
    "\n",
    "# ========== PART 3: EMBEDDING GENERATION ==========\n",
    "print(\"\\nüî§ PART 3: EMBEDDING GENERATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Note: Embedding requires sentence-transformers package.\")\n",
    "print(\"For this demonstration, we'll show the process.\")\n",
    "print(\"\\nTo generate real embeddings, run:\")\n",
    "print(\"pip install sentence-transformers\")\n",
    "print(\"Then use: from sentence_transformers import SentenceTransformer\")\n",
    "\n",
    "# Mock embeddings for demonstration\n",
    "print(f\"\\nüìä Demonstration: Would generate {len(all_chunks):,} embeddings\")\n",
    "print(f\"   Model: all-MiniLM-L6-v2 (384 dimensions)\")\n",
    "print(f\"   Total embedding size: {len(all_chunks) * 384 * 4 / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Create mock embedded chunks\n",
    "embedded_chunks = []\n",
    "for i, chunk_info in enumerate(chunk_metadata[:1000]):  # Just first 1000 for demo\n",
    "    embedded_chunks.append({\n",
    "        **chunk_info,\n",
    "        'embedding': [0.0] * 384,  # Mock 384-dim vector\n",
    "        'embedding_source': 'mock_demo'\n",
    "    })\n",
    "\n",
    "# Save mock embeddings\n",
    "embedded_path = \"../data/sampled/embedded_chunks_demo.json\"\n",
    "os.makedirs(os.path.dirname(embedded_path), exist_ok=True)\n",
    "with open(embedded_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(embedded_chunks[:100], f, ensure_ascii=False, indent=2)  # Save only 100\n",
    "\n",
    "print(f\"üíæ Saved demo embeddings to: {embedded_path}\")\n",
    "print(\"‚ö†Ô∏è  Note: These are MOCK embeddings for demonstration only\")\n",
    "\n",
    "# ========== PART 4: VECTOR STORE ==========\n",
    "print(\"\\nüóÑÔ∏è  PART 4: VECTOR STORE CREATION\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Vector store options:\")\n",
    "print(\"1. ChromaDB (recommended)\")\n",
    "print(\"2. FAISS\")\n",
    "print(\"3. Use pre-built vector store for Tasks 3-4\")\n",
    "\n",
    "print(\"\\nFor this task, we create a custom vector store.\")\n",
    "print(\"For Tasks 3-4, use the pre-built store with 1.37M chunks.\")\n",
    "\n",
    "# Create a simple vector store file structure\n",
    "vector_store_dir = \"../vector_store/custom\"\n",
    "os.makedirs(vector_store_dir, exist_ok=True)\n",
    "\n",
    "# Create a metadata file\n",
    "vector_store_info = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"total_chunks\": len(all_chunks),\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embedding_dimension\": 384,\n",
    "    \"sample_size\": len(sample_df),\n",
    "    \"note\": \"This is a demonstration vector store. Use pre-built for full dataset.\"\n",
    "}\n",
    "\n",
    "info_path = os.path.join(vector_store_dir, \"vector_store_info.json\")\n",
    "with open(info_path, 'w') as f:\n",
    "    json.dump(vector_store_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Created vector store structure at: {vector_store_dir}\")\n",
    "print(f\"üíæ Vector store info saved to: {info_path}\")\n",
    "\n",
    "# ========== TASK 2 SUMMARY ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 2 SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä STATISTICS:\")\n",
    "print(f\"  ‚Ä¢ Processed complaints: {len(sample_df):,}\")\n",
    "print(f\"  ‚Ä¢ Text chunks created: {len(all_chunks):,}\")\n",
    "print(f\"  ‚Ä¢ Average chunks/complaint: {len(all_chunks)/len(sample_df):.2f}\")\n",
    "print(f\"  ‚Ä¢ Chunk size: 500 characters\")\n",
    "print(f\"  ‚Ä¢ Chunk overlap: 50 characters\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES:\")\n",
    "print(f\"  1. {sample_path}\")\n",
    "print(f\"  2. {chunks_path}\")\n",
    "print(f\"  3. {embedded_path}\")\n",
    "print(f\"  4. {vector_store_dir}/\")\n",
    "\n",
    "print(f\"\\n‚úÖ TASK 2 COMPLETE!\")\n",
    "print(\"\\nüéØ NEXT STEPS:\")\n",
    "print(\"1. For real embeddings: pip install sentence-transformers chromadb\")\n",
    "print(\"2. To use the pre-built vector store: vector_store/prebuilt/\")\n",
    "print(\"3. Proceed to Task 3: RAG pipeline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f453a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
